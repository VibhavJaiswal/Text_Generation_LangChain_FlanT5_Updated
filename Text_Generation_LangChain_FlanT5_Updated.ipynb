{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSNe5dGyCZlq"
   },
   "source": [
    "#**Demo: Building a Text Generation Pipeline with LangChain and Hugging Face's Flan T5 Large Model**\n",
    "\n",
    "In this demo, you will learn how to create a Langchain HuggingFacePipeline for efficient text generation and dive into the creation of a Langchain chain to craft context-aware responses using a custom template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GIv4H_3Ce2F"
   },
   "source": [
    "##**Steps to Perform:**\n",
    "1. Install the Required Libraries and Dependencies\n",
    "2. Authenticate the Hugging Face Account and Set the API Key\n",
    "3. Use the Hugging Face Hub to Load the Flan T5 Large model\n",
    "4. Create a Langchain HuggingFacePipeline for Text Generation\n",
    "5. Build a Chain Using Langchain\n",
    "6. Test and Run the Chain on Few a Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMiJv-SkCo9e"
   },
   "source": [
    "###**Step 1: Install the Required Libraries and Dependencies**\n",
    "\n",
    "\n",
    "*   Install the necessary libraries, including **Langchain**, **Transformers**, and **Torch**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7Npz_xvrIxm3"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip2zy0mDC93N"
   },
   "source": [
    "# **Step 2: Authenticate the Hugging Face Account and Set the API Key**\n",
    "\n",
    "*   Click this link: https://huggingface.co/settings/tokens\n",
    "*   Login or create an account.\n",
    "*   Click on **New token**.\n",
    "*   On the dialog box, give a name and select the role as **write**.\n",
    "*   Copy the access token or API key.\n",
    "*   Replace **Your_HF_Key** with the copied key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "d_my4ywipYl1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"os.getenv(\"HF_TOKEN\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veodvSN0Dc7r"
   },
   "source": [
    "###**Step 3: Use the Hugging Face Hub to Load the Flan T5 Large model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "n4tjani-Zsd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 16:28:03.665211: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-11 16:28:03.715454: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 16:28:06.273940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 16:28:06.282941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-11 16:28:06.286212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC-NOTICE: GPU memory for this assignment is capped at 2048MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '1.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "\n",
    "# gpt = OpenAI(openai_api_key = \"Your Key\")\n",
    "\n",
    "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":1, \"max_length\":50})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wE90-mP9FHCJ"
   },
   "source": [
    "###**Step 4: Create a Langchain HuggingFacePipeline for Text Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4sfC8putZ4NX"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name=PromptTemplate(input_variables=[\"currency\"],\n",
    "                                    template=\"can you tell me the country to which this currency {currency} belongs to?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = prompt_template_name.format(currency=\"yen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6PN8DGuFSIn"
   },
   "source": [
    "# Creating Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eMzZa8lmqF_i"
   },
   "outputs": [],
   "source": [
    "llm_chain1 = LLMChain(prompt = prompt_template_name, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCy0qU74Ff4p"
   },
   "source": [
    "###**Step 6: Test and Run the Chain on Few a Questions**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VzJK0rXNqKwA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spain'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 1\n",
    "question = \"euro\"\n",
    "\n",
    "llm_chain1.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step. Finish the answer in 25 words\"\"\"\n",
    "\n",
    "prompt_template_name1 = PromptTemplate(template=template, input_variables=[\"in 25 words answer the question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain2 = LLMChain(prompt=prompt_template_name1, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Climate change is caused by changes in the atmosphere. Climate change is caused by changes in the atmosphere. Climate change is caused by changes in the atmosphere.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 2\n",
    "question = \"explain, What are the main causes of climate change, and how can we address them?\"\n",
    "\n",
    "llm_chain2.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UdK6ZB2GBMM0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial intelligence has been around for a long time. The first computer programs were written in the 1940s. The first computer programs were written in the 1950s.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question 3\n",
    "question = \"Provide a brief overview of the history of artificial intelligence.\"\n",
    "\n",
    "llm_chain2.run(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name2 = PromptTemplate(input_variables=['cuisine'],\n",
    "                                 template = \"i want to open a restaurant for {cuisine} food, suggest a fancy name for this restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['cuisine'], template='i want to open a restaurant for {cuisine} food, suggest a fancy name for this restaurant')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_name2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain3=LLMChain(llm=llm, prompt=prompt_template_name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The American'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain3.run(\"american\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Template\n",
    "prompt_template_name = PromptTemplate(input_variables=[\"industry\"],\n",
    "                                    template = \"I want to start a startup for {industry} , suggest me a good name for this\")\n",
    "\n",
    "name_chain = LLMChain(llm = llm, prompt = prompt_template_name)\n",
    "\n",
    "# Second Template\n",
    "prompt_template_items = PromptTemplate( input_variables = [\"name\"],\n",
    "                                     template = \"suggest 3 business strategies in bullet points for {name}\")\n",
    "\n",
    "strategies_chain = LLMChain(llm = llm, prompt = prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SimpleSequentialChain(chains=[name_chain, strategies_chain]) # This gives us one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Invest in a logistics startup - Invest in a logistics startup\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"logistics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 1\n",
    "prompt_template_name=PromptTemplate(input_variables=[\"cuisine\"],\n",
    "                                    template=\"i want to open a restaurant for this {cuisine}, suggest a fancy name for it\")\n",
    "\n",
    "name_chain=LLMChain(llm=llm, prompt=prompt_template_name,output_key=\"restaurant_name\")\n",
    " \n",
    "# Prompt 2\n",
    "prompt_templates_items=PromptTemplate(input_variables=[\"restaurant_name\"],\n",
    "                                      template=\"suggest 3 fancy and unique menu items for this {restaurant_name}\")\n",
    "food_items_chain=LLMChain(llm=llm,prompt=prompt_templates_items, output_key=\"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=SequentialChain(\n",
    "    chains=[name_chain, food_items_chain], input_variables=[\"cuisine\"],\n",
    "    output_variables=[\"restaurant_name\", \"menu_items\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'mexican', 'restaurant_name': 'Mexican', 'menu_items': 'guacamole'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"cuisine\": \"mexican\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using Chain i have connect multiple thing"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
